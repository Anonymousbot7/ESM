{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import pandas as pd\n",
    "# 定义输入和输出文件夹\n",
    "input_folder = \"data2\"  # CSV.gz 文件所在目录\n",
    "output_folder = \"data\"  # Parquet 存放目录\n",
    "\n",
    "# 确保输出目录存在\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# # # 遍历 data2 目录下所有 .csv.gz 文件\n",
    "# for file_name in os.listdir(input_folder):\n",
    "#     if file_name.endswith(\".csv.gz\"):  # 仅处理 CSV.gz 文件\n",
    "#         input_path = os.path.join(input_folder, file_name)\n",
    "#         output_path = os.path.join(output_folder, file_name.replace(\".csv.gz\", \".parquet\"))\n",
    "\n",
    "#         print(f\"正在转换: {input_path} -> {output_path}\")\n",
    "        \n",
    "#         # 读取 CSV.gz 为 PyArrow 表（不会一次性加载到内存）\n",
    "#         table = pv.read_csv(input_path)\n",
    "        \n",
    "#         # 保存为 Parquet\n",
    "#         pq.write_table(table, output_path)\n",
    "\n",
    "# print(\"所有文件已转换完成\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _parse_slice(s):\n",
    "    \"\"\"Convert int/slice string to proper slice object\"\"\"\n",
    "    if isinstance(s, int):\n",
    "        return slice(s, s+1)  # Single item slice\n",
    "    elif isinstance(s, str):\n",
    "        if ':' in s:\n",
    "            parts = [int(x) if x else None for x in s.split(':')]\n",
    "            return slice(*parts)\n",
    "        else:\n",
    "            return slice(int(s), int(s)+1)\n",
    "    return s\n",
    "\n",
    "def investigate(table, row=None, col=None):\n",
    "    \"\"\"\n",
    "    Analyze parquet file with flexible slicing:\n",
    "    - row/col can be int (single item) or slice spec (e.g., '2:5')\n",
    "    - Returns: (submatrix, metadata) where metadata is (colnames, nrow, ncol)\n",
    "    \n",
    "    Example usage:\n",
    "    investigate(\"data.parquet\", row=1, col=1)  # 2nd row & 2nd column\n",
    "    investigate(\"data.parquet\", row=\"1:4\")     # rows 2-4 (Python slice)\n",
    "    \"\"\"\n",
    "    # Read entire table\n",
    "    nrow, ncol = table.num_rows, table.num_columns\n",
    "    \n",
    "    # Parse slicing parameters\n",
    "    row_slice = _parse_slice(row) if row is not None else slice(None)\n",
    "    col_slice = _parse_slice(col) if col is not None else slice(None)\n",
    "    \n",
    "    # Handle column selection by index/name\n",
    "    if isinstance(col_slice, int):\n",
    "        col_idx = col_slice\n",
    "        if col_idx >= ncol:\n",
    "            raise ValueError(f\"Column index {col_idx} out of range ({ncol} cols)\")\n",
    "        col_names = [table.column_names[col_idx]]\n",
    "    else:\n",
    "        col_names = table.column_names[col_slice]\n",
    "    \n",
    "    # Slice the table\n",
    "    sub_table = table.slice(row_slice.start, row_slice.stop).select(col_names)\n",
    "    \n",
    "    # Convert to pandas for display\n",
    "    df = sub_table.to_pandas()\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = (table.column_names, nrow, ncol)\n",
    "    \n",
    "    return df, metadata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "\n",
    "lab = pq.read_table('data/lab.parquet')\n",
    "apachePatientResult=pq.read_table('data/apachePatientResult.parquet')\n",
    "\n",
    "apacheApsVar = pq.read_table('data/apacheApsVar.parquet')\n",
    "\n",
    "patient = pq.read_table('data/patient.parquet')\n",
    "# nurseAssessment = pq.read_table('data/nurseAssessment.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()  # 手动清理内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patient  unitdischargeoffset ->Y\n",
    "一个patient进入icu次数  unitvisitnumber\n",
    "\n",
    "sample_n\n",
    "patient: subclass desease apacheadmissiondx挑选病理种类得到subsample\n",
    "\n",
    "hospital id, give subsample*****\n",
    "X selection:\n",
    "patient age, gender, ethnicity人种,admissionweight,dischargeheight,\n",
    "\n",
    "vitalperidic, 第一次测量的各种值\n",
    "\n",
    "lab\n",
    "apachpatientresult: apachescore,select apachversion IVa, predictedicumortality,predictediculos\n",
    "\n",
    "一个病人会有很多病，选择第一次进ICU的病,在patient中有.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取关键数据\n",
    "dfpatient,metapatient=investigate(patient,row=\"0:3000000\")\n",
    "dflab,metalab=investigate(lab,row=\"0:40000000\")\n",
    "dfAPR,metaAPR=investigate(apachePatientResult,row=\"0:3000000\")\n",
    "most_frequent_id = dfpatient[\"hospitalid\"].value_counts().idxmax()\n",
    "\n",
    "# 提取同一 hospitalid 的数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59,  60,  73,  67,  66,  63,  71,  56,  68,  69,  58,  61,  92,\n",
       "        79, 108,  83,  95,  84,  93,  86,  90,  85, 102,  96,  94,  91,\n",
       "       110, 122, 123, 125, 135, 112, 133, 120, 131, 138, 115, 136, 144,\n",
       "       143, 142, 140, 141, 175, 165, 152, 167, 171, 154, 148, 157, 176,\n",
       "       164, 155, 146, 174, 158, 179, 151, 156, 184, 181, 183, 182, 180,\n",
       "       188, 196, 197, 198, 194, 195, 199, 202, 203, 201, 206, 205, 200,\n",
       "       204, 208, 210, 207, 209, 217, 224, 220, 215, 212, 226, 227, 244,\n",
       "       245, 243, 246, 252, 251, 248, 253, 249, 250, 264, 266, 258, 256,\n",
       "       267, 259, 262, 254, 263, 265, 272, 281, 271, 283, 269, 279, 268,\n",
       "       277, 280, 282, 275, 273, 312, 307, 300, 318, 310, 301, 303, 323,\n",
       "       338, 345, 331, 337, 342, 328, 336, 350, 352, 351, 365, 360, 364,\n",
       "       358, 353, 355, 357, 356, 363, 361, 394, 400, 397, 383, 390, 382,\n",
       "       391, 381, 388, 389, 387, 403, 384, 402, 407, 393, 399, 398, 405,\n",
       "       408, 404, 386, 396, 392, 401, 409, 385, 411, 413, 412, 414, 417,\n",
       "       416, 420, 424, 423, 419, 425, 422, 421, 439, 435, 428, 438, 440,\n",
       "       429, 437, 433, 434, 436, 443, 445, 444, 452, 449, 447, 459, 458])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfpatient['hospitalid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 307 6 15\n",
      "60 197 5 11\n",
      "73 3910 207 68\n",
      "67 231 2 7\n",
      "66 561 7 7\n",
      "63 623 25 45\n",
      "71 335 11 4\n",
      "56 151 1 3\n",
      "68 240 3 5\n",
      "69 281 5 9\n",
      "58 147 3 2\n",
      "61 118 1 3\n",
      "92 435 12 1\n",
      "79 1043 61 7\n",
      "108 293 6 0\n",
      "83 0 0 0\n",
      "95 222 2 0\n",
      "84 0 0 0\n",
      "93 0 0 0\n",
      "86 0 0 0\n",
      "90 0 0 0\n",
      "85 0 0 0\n",
      "102 24 0 0\n",
      "96 1 0 0\n",
      "94 5 0 0\n",
      "91 7 0 0\n",
      "110 0 0 0\n",
      "122 0 0 0\n",
      "123 0 0 0\n",
      "125 0 0 0\n",
      "135 0 0 0\n",
      "112 0 0 0\n",
      "133 0 0 0\n",
      "120 0 0 0\n",
      "131 0 0 0\n",
      "138 0 0 0\n",
      "115 0 0 0\n",
      "136 0 0 0\n",
      "144 575 2 0\n",
      "143 546 9 1\n",
      "142 415 2 0\n",
      "140 495 2 0\n",
      "141 1177 5 1\n",
      "175 0 0 0\n",
      "165 170 12 8\n",
      "152 129 1 4\n",
      "167 310 5 19\n",
      "171 102 1 3\n",
      "154 92 1 3\n",
      "148 172 4 2\n",
      "157 133 0 9\n",
      "176 266 5 10\n",
      "164 15 0 0\n",
      "155 25 10 3\n",
      "146 74 3 4\n",
      "174 9 0 0\n",
      "158 25 0 0\n",
      "179 0 0 0\n",
      "151 0 0 0\n",
      "156 0 0 0\n",
      "184 734 33 5\n",
      "181 794 43 7\n",
      "183 1291 95 29\n",
      "182 252 5 9\n",
      "180 354 12 2\n",
      "188 1476 1046 77\n",
      "196 269 3 0\n",
      "197 687 22 8\n",
      "198 253 9 2\n",
      "194 288 5 1\n",
      "195 505 32 5\n",
      "199 683 45 12\n",
      "202 121 5 2\n",
      "203 113 0 0\n",
      "201 120 2 0\n",
      "206 379 7 5\n",
      "205 0 0 0\n",
      "200 230 5 2\n",
      "204 89 2 0\n",
      "208 0 0 0\n",
      "210 0 0 0\n",
      "207 0 0 0\n",
      "209 0 0 0\n",
      "217 278 6 1\n",
      "224 22 0 0\n",
      "220 179 4 0\n",
      "215 146 4 1\n",
      "212 0 0 0\n",
      "226 664 41 20\n",
      "227 1022 68 9\n",
      "244 0 0 0\n",
      "245 0 0 0\n",
      "243 0 0 0\n",
      "246 0 0 0\n",
      "252 2619 134 14\n",
      "251 262 3 0\n",
      "248 1478 44 2\n",
      "253 383 19 1\n",
      "249 294 7 0\n",
      "250 199 0 0\n",
      "264 2791 64 8\n",
      "266 187 1 0\n",
      "258 244 1 0\n",
      "256 477 17 0\n",
      "267 27 0 0\n",
      "259 447 10 1\n",
      "262 144 3 0\n",
      "254 0 0 0\n",
      "263 0 0 0\n",
      "265 0 0 0\n",
      "272 0 0 0\n",
      "281 0 0 0\n",
      "271 736 47 3\n",
      "283 0 0 0\n",
      "269 0 0 0\n",
      "279 655 12 2\n",
      "268 0 0 0\n",
      "277 0 0 0\n",
      "280 0 0 0\n",
      "282 0 0 0\n",
      "275 0 0 0\n",
      "273 0 0 0\n",
      "312 513 42 3\n",
      "307 896 536 49\n",
      "300 620 335 71\n",
      "318 723 26 2\n",
      "310 71 55 15\n",
      "301 647 22 2\n",
      "303 129 7 1\n",
      "323 8 1 0\n",
      "338 123 66 9\n",
      "345 477 636 38\n",
      "331 429 318 39\n",
      "337 0 0 0\n",
      "342 0 0 0\n",
      "328 128 3 1\n",
      "336 278 255 15\n",
      "350 0 0 0\n",
      "352 0 0 0\n",
      "351 4 0 0\n",
      "365 1085 44 5\n",
      "360 27 1 0\n",
      "364 238 5 0\n",
      "358 257 5 1\n",
      "353 535 17 0\n",
      "355 0 0 0\n",
      "357 0 0 0\n",
      "356 12 0 0\n",
      "363 57 1 0\n",
      "361 0 0 0\n",
      "394 1684 46 2\n",
      "400 1347 84 16\n",
      "397 0 0 0\n",
      "383 352 14 0\n",
      "390 858 23 6\n",
      "382 0 0 0\n",
      "391 0 0 0\n",
      "381 203 3 0\n",
      "388 808 25 2\n",
      "389 386 9 1\n",
      "387 473 27 5\n",
      "403 513 27 2\n",
      "384 0 0 0\n",
      "402 365 12 0\n",
      "407 272 8 0\n",
      "393 118 3 0\n",
      "399 175 2 0\n",
      "398 250 5 0\n",
      "405 246 8 1\n",
      "408 0 0 0\n",
      "404 366 14 0\n",
      "386 380 13 1\n",
      "396 520 14 1\n",
      "392 544 25 2\n",
      "401 0 0 0\n",
      "409 0 0 0\n",
      "385 0 0 0\n",
      "411 65 6 1\n",
      "413 168 11 0\n",
      "412 27 1 0\n",
      "414 0 0 0\n",
      "417 870 24 7\n",
      "416 1075 48 11\n",
      "420 2698 131 25\n",
      "424 360 0 0\n",
      "423 0 0 0\n",
      "419 0 0 0\n",
      "425 0 0 0\n",
      "422 303 5 0\n",
      "421 510 12 0\n",
      "439 293 6 0\n",
      "435 945 37 9\n",
      "428 206 5 1\n",
      "438 157 4 0\n",
      "440 0 0 0\n",
      "429 223 6 1\n",
      "437 47 2 0\n",
      "433 0 0 0\n",
      "434 270 6 1\n",
      "436 50 1 0\n",
      "443 1582 94 20\n",
      "445 0 0 0\n",
      "444 1096 37 2\n",
      "452 568 34 8\n",
      "449 0 0 0\n",
      "447 24 2 0\n",
      "459 462 15 1\n",
      "458 2178 86 19\n"
     ]
    }
   ],
   "source": [
    "# for i in dfpatient['hospitalid'].unique():\n",
    "#     dfpatient1 = dfpatient[dfpatient[\"hospitalid\"] == i]\n",
    "#     dfpatient1 = dfpatient1.dropna(subset=[\"admissionheight\"])\n",
    "#     dfpatient1 = dfpatient1.dropna(subset=[\"admissionweight\"])\n",
    "#         # Merge dfpatient and df_pivot_clean on 'patientunitstayid'\n",
    "#     merged_df = dfpatient1.merge(df_pivotlab_clean, on='patientunitstayid', how='inner')\n",
    "\n",
    "#     # Merge the result with df_APR_clean on 'patientunitstayid'\n",
    "#     final_df = merged_df.merge(df_APR_clean, on='patientunitstayid', how='inner')\n",
    "\n",
    "#     # Display the final merged dataframe\n",
    "#     final_df['apacheadmissiondx']\n",
    "\n",
    "#     # final_df\n",
    "#     final_df\n",
    "\n",
    "#     # nltk.download('punkt')\n",
    "#     # nltk.download('averaged_perceptron_tagger')\n",
    "#     # nltk.download('wordnet')\n",
    "    \n",
    "#     selected_column=['patientunitstayid', 'unitvisitnumber',  'gender', 'age',\n",
    "#         'ethnicity',  'apacheadmissiondx',\n",
    "#         'admissionheight', \n",
    "#             'unitadmitsource', \n",
    "#         'admissionweight', 'sodium', 'creatinine', 'BUN', 'potassium', 'Hct',\n",
    "#         'glucose', 'chloride', 'WBC x 1000', 'Hgb', 'RBC', 'calcium',\n",
    "#         'platelets x 1000', 'MCV', 'MCHC', 'bicarbonate', 'RDW', 'MCH',\n",
    "#         'anion gap', '-lymphs', '-monos', 'predictedicumortality',\n",
    "#         'predictediculos', 'actualiculos', 'predictedhospitalmortality',\n",
    "#         'predictedhospitallos', 'actualhospitallos']\n",
    "#     df=final_df[selected_column]\n",
    "#     print(i,(df['unitvisitnumber'] == 1).sum(),(df['unitvisitnumber'] == 2).sum(),(df['unitvisitnumber'] == 3).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选择医院代号188,不可在不同医院之间预测ICU次数\n",
    "\n",
    "dfpatient1 = dfpatient[dfpatient[\"hospitalid\"] == 71]\n",
    "dfpatient1 = dfpatient1.dropna(subset=[\"admissionheight\"])\n",
    "dfpatient1 = dfpatient1.dropna(subset=[\"admissionweight\"])\n",
    "# dfpatient1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean lab and APR table\n",
    "df_pivot_lab = dflab.pivot_table(index=\"patientunitstayid\", columns=\"labname\", values=\"labresult\", aggfunc=\"first\").reset_index()\n",
    "\n",
    "def clean_df(df, K=20):\n",
    "\n",
    "    # 确保存在 'patientunitstayid' 列\n",
    "    if 'patientunitstayid' not in df.columns:\n",
    "        raise ValueError(\"DataFrame 必须包含 'patientunitstayid' 列\")\n",
    "\n",
    "    # 排除 'patientunitstayid'，计算其他列的 NA 数量\n",
    "    columns_to_consider = df.columns.drop('patientunitstayid')\n",
    "    if len(columns_to_consider) < K:\n",
    "        raise ValueError(f\"可用列数 ({len(columns_to_consider)}) 不足以选择 K={K} 列\")\n",
    "\n",
    "    # 计算各列的 NA 数量并排序\n",
    "    na_counts = df[columns_to_consider].isna().sum()\n",
    "    selected_columns = na_counts.nsmallest(K).index.tolist()\n",
    "\n",
    "    # 构建新 DataFrame\n",
    "    cleaned_df = df[['patientunitstayid'] + selected_columns]\n",
    "\n",
    "    # 过滤选中的 K 列中所有 NA 的行\n",
    "    cleaned_df = cleaned_df.dropna(subset=selected_columns)\n",
    "\n",
    "    return cleaned_df\n",
    "df_pivotlab_clean=clean_df(df_pivot_lab,K=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_df = dfAPR[dfAPR[\"apacheversion\"] == \"IVa\"]\n",
    "lsit_columns=['predictedicumortality',\n",
    "       'predictediculos', 'actualiculos', 'predictedhospitalmortality', 'predictedhospitallos', 'actualhospitallos']\n",
    "\n",
    "\n",
    "df_APR_clean=filtered_df[['patientunitstayid'] + lsit_columns]\n",
    "df_APR_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/xuranm/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#得到最终的数据表格，但未完成category factor的转化\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "# Merge dfpatient and df_pivot_clean on 'patientunitstayid'\n",
    "merged_df = dfpatient1.merge(df_pivotlab_clean, on='patientunitstayid', how='inner')\n",
    "\n",
    "# Merge the result with df_APR_clean on 'patientunitstayid'\n",
    "final_df = merged_df.merge(df_APR_clean, on='patientunitstayid', how='inner')\n",
    "\n",
    "# Display the final merged dataframe\n",
    "final_df['apacheadmissiondx']\n",
    "\n",
    "# final_df\n",
    "final_df\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "final_df.columns\n",
    "selected_column=['patientunitstayid', 'unitvisitnumber',  'gender', 'age',\n",
    "       'ethnicity',  'apacheadmissiondx',\n",
    "       'admissionheight', \n",
    "        'unitadmitsource', \n",
    "       'admissionweight', 'sodium', 'creatinine', 'BUN', 'potassium', 'Hct',\n",
    "       'glucose', 'chloride', 'WBC x 1000', 'Hgb', 'RBC', 'calcium',\n",
    "       'platelets x 1000', 'MCV', 'MCHC', 'bicarbonate', 'RDW', 'MCH',\n",
    "       'anion gap', '-lymphs', '-monos', 'predictedicumortality',\n",
    "       'predictediculos', 'actualiculos', 'predictedhospitalmortality',\n",
    "       'predictedhospitallos', 'actualhospitallos']\n",
    "\n",
    "df=final_df[selected_column]\n",
    "print((df['unitvisitnumber'] == 1).sum())\n",
    "print((df['unitvisitnumber'] == 2).sum())\n",
    "(df['unitvisitnumber'] == 3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['apacheadmissiondx']\n",
    "num_unique = df['apacheadmissiondx'].nunique()\n",
    "num_unique\n",
    "\n",
    "word_list = []\n",
    "for row in df['apacheadmissiondx']:\n",
    "    if pd.notna(row):  # 忽略 NaN 值\n",
    "        words = row.split()  # 按空格拆分单词（自动过滤空字符串）\n",
    "        # 去重并保持原始顺序\n",
    "        seen = set()\n",
    "        unique_words = [word for word in words if not (word in seen or seen.add(word))]\n",
    "        word_list.extend(unique_words)\n",
    "\n",
    "# 词性标注\n",
    "tagged_words = nltk.pos_tag(word_list)\n",
    "\n",
    "# 过滤出名词和形容词\n",
    "nouns_and_adjs = [\n",
    "    word for word, tag in tagged_words\n",
    "    if tag.startswith('NN') or tag.startswith('JJ')\n",
    "]\n",
    "\n",
    "# 统计频率\n",
    "word_freq = Counter(nouns_and_adjs)\n",
    "word_freq\n",
    "\n",
    "K = 60\n",
    "\n",
    "# 筛选频率大于 K 的单词\n",
    "filtered_word_freq = {word: freq for word, freq in word_freq.items() if freq > K}\n",
    "\n",
    "filtered_word_freq\n",
    "\n",
    "\n",
    "def filter_same_freq_words(filtered_word_freq, df):\n",
    "    # 按频率分组\n",
    "    freq_to_words = defaultdict(list)\n",
    "    for word, freq in filtered_word_freq.items():\n",
    "        freq_to_words[freq].append(word)\n",
    "\n",
    "    # 记录需要保留的单词\n",
    "    words_to_keep = set()\n",
    "\n",
    "    # 检查每组频率相同的单词是否总是出现在相同的行中\n",
    "    for freq, words in freq_to_words.items():\n",
    "        if len(words) >= 1:  # 只检查频率相同的多个单词\n",
    "            # 创建单词到行索引的映射\n",
    "            word_to_rows = defaultdict(set)\n",
    "            for idx, row in df['apacheadmissiondx'].items():\n",
    "                if pd.notna(row):\n",
    "                    row_words = row.split()\n",
    "                    for word in words:\n",
    "                        if word in row_words:\n",
    "                            word_to_rows[word].add(idx)\n",
    "            # 检查所有单词的行索引是否相同\n",
    "            first_word_rows = word_to_rows[words[0]]\n",
    "            all_same = True\n",
    "            for word in words:\n",
    "                if word_to_rows[word] != first_word_rows:\n",
    "                    all_same = False\n",
    "                    break\n",
    "            # 如果所有单词的行索引相同，则只保留第一个单词\n",
    "            if all_same:\n",
    "                words_to_keep.add(words[0])\n",
    "            else:\n",
    "                words_to_keep.update(words)\n",
    "        else:  # 频率唯一的单词直接保留\n",
    "            words_to_keep.add(words[0])\n",
    "\n",
    "    # 构建结果字典，只保留需要保留的单词\n",
    "    result = {word: freq for word, freq in filtered_word_freq.items() if word in words_to_keep}\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dictionary=filter_same_freq_words(filtered_word_freq,df)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contains_word(a, b):\n",
    "    # Match the word `a` even if surrounded by punctuation/whitespace\n",
    "    pattern = rf'(?<!\\w){re.escape(a)}(?!\\w)'  # More robust check\n",
    "    return bool(re.search(pattern, b))\n",
    "\n",
    "\n",
    "df_new = df.copy()\n",
    "word_list=list(dictionary.keys())\n",
    "word_length = len(word_list)\n",
    "vector_length = word_length + 1\n",
    "apache_vectors = []\n",
    "for row in df['apacheadmissiondx']:\n",
    "    # print(row)\n",
    "    vector = [0] * vector_length\n",
    "    for word in word_list:\n",
    "        if contains_word(word,row):\n",
    "            vector[word_list.index(word)]=1\n",
    "    apache_vectors.append(vector)\n",
    "\n",
    "# 添加到新的 DataFrame\n",
    "for i in range(vector_length):\n",
    "    column_name = f\"{'apacheadmissiondx'}_{i}\"\n",
    "    df_new[column_name] = [v[i] for v in apache_vectors]\n",
    "\n",
    "# 删除原始的 apacheadmissiondx 列\n",
    "df_new.drop('apacheadmissiondx', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def contains_word(a, b):\n",
    "    pattern = rf'\\b{re.escape(a)}\\b'\n",
    "    return bool(re.search(pattern, b))\n",
    "\n",
    "\n",
    "def one_hot_encode_categorical(df, category_column_name, word_list):\n",
    "    df_new = df.copy()\n",
    "\n",
    "    for column in category_column_name:\n",
    "        if column == 'apacheadmissiondx':\n",
    "            # 处理 apacheadmissiondx 列\n",
    "            word_length = len(word_list)\n",
    "            vector_length = word_length + 1\n",
    "\n",
    "            # 生成对应的向量\n",
    "            apache_vectors = []\n",
    "            for row in df[column]:\n",
    "                vector = [0] * vector_length\n",
    "                \n",
    "                apache_vectors.append(vector)\n",
    "\n",
    "            # 添加到新的 DataFrame\n",
    "            for i in range(vector_length):\n",
    "                column_name = f\"{column}_{i}\"\n",
    "                df_new[column_name] = [v[i] for v in apache_vectors]\n",
    "\n",
    "            # 删除原始的 apacheadmissiondx 列\n",
    "            df_new.drop(column, axis=1, inplace=True)\n",
    "        else:\n",
    "            # 处理普通列，转换为独热编码\n",
    "            unique_values = df[column].unique()\n",
    "            for value in unique_values:\n",
    "                column_name = f\"{column}_{value}\"\n",
    "                df_new[column_name] = (df[column] == value).astype(int)\n",
    "            # 删除原始的列\n",
    "            df_new.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "\n",
    "['patientunitstayid', 'unitvisitnumber',  'gender', 'age',\n",
    "       'ethnicity',  'apacheadmissiondx',\n",
    "       'admissionheight', \n",
    "        'unitadmitsource', \n",
    "       'admissionweight', 'sodium', 'creatinine', 'BUN', 'potassium', 'Hct',\n",
    "       'glucose', 'chloride', 'WBC x 1000', 'Hgb', 'RBC', 'calcium',\n",
    "       'platelets x 1000', 'MCV', 'MCHC', 'bicarbonate', 'RDW', 'MCH',\n",
    "       'anion gap', '-lymphs', '-monos', 'predictedicumortality',\n",
    "       'predictediculos', 'actualiculos', 'predictedhospitalmortality',\n",
    "       'predictedhospitallos', 'actualhospitallos']\n",
    "\n",
    "dfnew=one_hot_encode_categorical(df,['gender', 'ethnicity','unitadmitsource'],dictionary)\n",
    "dfnew.apacheadmissiondx.to_csv(\"1.csv\")\n",
    "dfnew.loc[dfnew[\"age\"] =='> 89', \"age\"] = 89\n",
    "dfnew.to_csv(\"output.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew.loc[dfnew[\"age\"] =='> 89', \"age\"] = 89\n",
    "dfnew.to_csv(\"output.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define category mapping\n",
    "category_map = {\n",
    "    \"ARDS\": \"Respiratory\",\n",
    "    \"pneumonia\": \"Respiratory\",\n",
    "    \"post-extubation edema\": \"Respiratory\",\n",
    "    \"bacteremia\": \"Sepsis-related\",\n",
    "    \"infection\": \"Sepsis-related\",\n",
    "    \"hemoptysis\": \"Respiratory\",\n",
    "    \"tracheostomy\": \"Respiratory\",\n",
    "    \"Cardiac arrest\": \"Cardiac\",\n",
    "    \"CHF\": \"Cardiac\",\n",
    "    \"Infarction\": \"Cardiac\",\n",
    "    \"hypertension\": \"Cardiac\",\n",
    "    \"stroke\": \"Neurological\",\n",
    "    \"Sepsis\": \"Sepsis-related\",\n",
    "    \"Septic shock\": \"Sepsis-related\",\n",
    "    \"CVA\": \"Neurological\",\n",
    "    \"coma\": \"Neurological\",\n",
    "    \"intracranial hemorrhage\": \"Neurological\",\n",
    "    \"seizure\": \"Neurological\",\n",
    "    \"subarachnoid hemorrhage\": \"Neurological\",\n",
    "    \"GI bleeding\": \"Gastrointestinal\",\n",
    "    \"peritonitis\": \"Gastrointestinal\",\n",
    "    \"variceal bleeding\": \"Gastrointestinal\",\n",
    "    \"surgery\": \"Gastrointestinal\",\n",
    "    \"Acid-base\": \"Metabolic/Electrolyte\",\n",
    "    \"electrolyte\": \"Metabolic/Electrolyte\",\n",
    "    \"diabetes\": \"Metabolic/Electrolyte\",\n",
    "    \"cocaine\": \"Toxicology\",\n",
    "    \"benzodiazepines\": \"Toxicology\",\n",
    "    \"calcium channel blockers\": \"Toxicology\",\n",
    "    \"analgesic\": \"Toxicology\",\n",
    "    \"trauma\": \"Surgical/Trauma\",\n",
    "    \"transphenoidal surgery\": \"Surgical/Trauma\"\n",
    "}\n",
    "\n",
    "# Function to categorize each diagnosis\n",
    "def categorize_diagnosis(diagnosis_str):\n",
    "    diagnoses = diagnosis_str.split(\", \")  # Split by comma\n",
    "    category_set = set()  # Use a set to avoid duplicates\n",
    "    \n",
    "    for diagnosis in diagnoses:\n",
    "        for key, category in category_map.items():\n",
    "            if key.lower() in diagnosis.lower():  # Case-insensitive matching\n",
    "                category_set.add(category)\n",
    "    \n",
    "    return \", \".join(category_set) if category_set else \"Other\"  # Combine categories\n",
    "\n",
    "# Apply function to create the new categorized column\n",
    "dfnew[\"apacheadmissiondx_category\"] = dfnew[\"apacheadmissiondx\"].apply(categorize_diagnosis)\n",
    "\n",
    "# Save to a new CSV\n",
    "dfnew.drop('apacheadmissiondx', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category_vectors = dfnew[\"apacheadmissiondx_category\"].str.get_dummies(sep=\", \")\n",
    "df_combined = pd.concat([dfnew, df_category_vectors], axis=1)\n",
    "\n",
    "# Drop the original apacheadmissiondx_category column\n",
    "df_combined = df_combined.drop(\"apacheadmissiondx_category\", axis=1)\n",
    "\n",
    "# Save the final transformed DataFrame\n",
    "df_combined.to_csv(\"final_transformed_data_458.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_combined.unitvisitnumber==3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
