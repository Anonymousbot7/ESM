{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class SampleSet:\n",
    "    def __init__(self, index,X, Y,id,module='Bernoulli'):\n",
    "        \"\"\"\n",
    "        Initializes the SampleSet with n samples and p features for X.\n",
    "        Y is generated based on the conditional probability P(Y=1|X).\n",
    "        \"\"\"\n",
    "        self.index=index\n",
    "        self.n=X.shape[0]\n",
    "        self.p=X.shape[1]\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.id=id\n",
    "        self.subtrain=None\n",
    "        self.subval=None\n",
    "        self.counts=None\n",
    "        self.r=int(self.n**index)\n",
    "\n",
    "    \n",
    "\n",
    "    def get_sample_set(self):\n",
    "        \"\"\"Returns the main sample set (X, Y).\"\"\"\n",
    "        return self.X, self.Y\n",
    "    \n",
    "    def get_sub_samples_with_validation(self, B):\n",
    "        \"\"\"\n",
    "        Generates B sub-sample sets, each containing r samples randomly selected \n",
    "        from the main sample set, along with corresponding validation sets.\n",
    "        \n",
    "        Also counts the number of times each index is selected across all B sub-samples.\n",
    "        \n",
    "        Returns:\n",
    "            train_samples: List of tuples, each containing (train_X, train_Y, train_indices)\n",
    "            validation_samples: List of tuples, each containing (val_X, val_Y, val_indices)\n",
    "            selection_counts: Dictionary with counts of each index's appearance in the B sub-samples.\n",
    "        \"\"\"\n",
    "        train_samples = []\n",
    "        validation_samples = []\n",
    "        selection_counts = Counter()  # To track appearances of each index\n",
    "        indices = torch.arange(self.n)\n",
    "        self.B=B\n",
    "        for _ in range(B):\n",
    "            # Randomly select r unique indices for the sub-sample\n",
    "            selected_indices = indices[torch.randperm(self.n)[:self.r]]\n",
    "            \n",
    "            # Update selection count for each index\n",
    "            selection_counts.update(selected_indices.tolist())\n",
    "            \n",
    "            # Get validation indices (those not in selected_indices)\n",
    "            val_indices = torch.tensor([i for i in indices if i not in selected_indices])\n",
    "\n",
    "            # Separate sub-sample and validation sets, including original indices\n",
    "            X_sub = self.X[selected_indices]\n",
    "            Y_sub = self.Y[selected_indices]\n",
    "            X_val = self.X[val_indices]\n",
    "            Y_val = self.Y[val_indices]\n",
    "            \n",
    "            # Append to train_samples and validation_samples lists\n",
    "            train_samples.append((X_sub, Y_sub, selected_indices))\n",
    "            validation_samples.append((X_val, Y_val, val_indices))\n",
    "        self.subtrain=train_samples\n",
    "        self.subval=validation_samples\n",
    "        self.counts=dict(selection_counts)\n",
    "        return train_samples, validation_samples, dict(selection_counts)\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        \"\"\"Saves the SampleSet instance to a file.\"\"\"\n",
    "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "        torch.save(self, file_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        \"\"\"Loads a SampleSet instance from a file.\"\"\"\n",
    "        return torch.load(file_path,weights_only=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"final_transformed_data_188.csv\")\n",
    "patient_ids = df[\"patientunitstayid\"]\n",
    "# Create binary target Y\n",
    "df[\"Y\"] = (df[\"unitvisitnumber\"] ).astype(float)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "df=df.drop(columns=[\"patientunitstayid\"])\n",
    "X = df.drop([\"unitvisitnumber\", \"Y\"], axis=1)\n",
    "y = df[\"Y\"].values-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuranm/.local/lib/python3.12/site-packages/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# To store the splits\n",
    "splits = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for train_idx, test_idx in skf.split(X, y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    id_train, id_test = patient_ids[train_idx], patient_ids[test_idx]\n",
    "    \n",
    "    # Append the split to the list\n",
    "    splits.append((X_train, X_test, y_train, y_test, id_train, id_test))\n",
    "\n",
    "# Now `splits` contains 5 tuples, each with (X_train, X_test, y_train, y_test, id_train, id_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Column indices (0-based)\n",
    "B=3000\n",
    "X_train=splits[0][0]\n",
    "X_test=splits[0][1]\n",
    "y_train=splits[0][2]\n",
    "y_test=splits[0][3]\n",
    "id_train=splits[0][4]\n",
    "id_test=splits[0][5]\n",
    "\n",
    "sample_set = SampleSet(0.9,X_train,y_train,id_train,module='Bernoulli')\n",
    "\n",
    "continuous_cols = [0] + list(range(7,13 ))+list(range(26,28 ))\n",
    "\n",
    "categorical_cols=list(range(37, 38))\n",
    "p=len(continuous_cols)+len(categorical_cols)\n",
    "\n",
    "# Loop through all 5 folds\n",
    "for i, (X_train, X_test, y_train, y_test, id_train, id_test) in enumerate(splits):\n",
    "    # Split features\n",
    "    X_train_cont = X_train.iloc[:, continuous_cols]\n",
    "    X_train_cat = X_train.iloc[:, categorical_cols]\n",
    "\n",
    "    X_test_cont = X_test.iloc[:, continuous_cols]\n",
    "    X_test_cat = X_test.iloc[:, categorical_cols]\n",
    "\n",
    "    # Initialize scaler using training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_cont_scaled = scaler.fit_transform(X_train_cont)\n",
    "\n",
    "    # Apply same transformation to test data\n",
    "    X_test_cont_scaled = scaler.transform(X_test_cont)\n",
    "\n",
    "    # Combine scaled continuous + original categorical features\n",
    "    X_train_processed = np.hstack([X_train_cont_scaled, X_train_cat])\n",
    "    X_test_processed = np.hstack([X_test_cont_scaled, X_test_cat])\n",
    "\n",
    "    # Convert to float32 tensors\n",
    "    X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Create and save training SampleSet\n",
    "    sample_set_train = SampleSet(0.9, X_train_tensor, y_train_tensor, id_train, module='Bernoulli')\n",
    "    sample_set_train.get_sub_samples_with_validation(B)\n",
    "    sample_set_train.save(f'sampleset/188/sampleset{p}poisson{B}_trainfolder{i}.pth')\n",
    "\n",
    "    # Create and save testing SampleSet\n",
    "    sample_set_test = SampleSet(0.9, X_test_tensor, y_test_tensor, id_test, module='Bernoulli')\n",
    "    sample_set_test.save(f'sampleset/188/sampleset{p}poisson{B}_testfolder{i}.pth')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
