{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "def f_1(x):\n",
    "    return x[:, 0] + 0.25 * x[:, 1] ** 2+0.1*torch.tanh(0.5*x[:,2]-0.3)\n",
    "\n",
    "\n",
    "# def f_2(x):\n",
    "#     return x[:,0]**2/2-abs(x[:,4]*x[:,9])+torch.exp(0.1*x[:,14])-torch.sin(3.141592*x[:,19])\n",
    "# def f_2(x):\n",
    "#     return 2*(x[:,0]>0)-2*(x[:,0]<0)\n",
    "\n",
    "def poisson_loss(logits,y_true):\n",
    "    \"\"\"\n",
    "    Compute the Poisson negative log-likelihood loss.\n",
    "    \n",
    "    Args:\n",
    "        y_true (torch.Tensor): True labels (0, 1, 2, ...), shape (batch_size,).\n",
    "        logits (torch.Tensor): Output of the DNN (before exponentiation), shape (batch_size,).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Mean negative log-likelihood loss over the batch.\n",
    "    \"\"\"\n",
    "    # Convert logits to λ(x) = e^logits\n",
    "    lambda_pred = torch.exp(logits)\n",
    "    \n",
    "    # Compute the negative log-likelihood\n",
    "    loss = lambda_pred - y_true * logits  # Equivalent to λ(x) - Y * log(λ(x))\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def binomial_loss(logits, batch_Y, batch_trial):\n",
    "    # Ensure batch_trial is a float tensor\n",
    "    batch_trial = batch_trial.float()\n",
    "    # Compute softplus for logits and -logits\n",
    "    softplus_pos = F.softplus(logits)\n",
    "    softplus_neg = F.softplus(-logits)\n",
    "    # Calculate the loss components\n",
    "    loss_elements = batch_Y * softplus_neg + (batch_trial - batch_Y) * softplus_pos\n",
    "    # Return the mean loss over the batch\n",
    "    return loss_elements.mean()\n",
    "\n",
    "\n",
    "class SampleSet:\n",
    "    def __init__(self, n, p,f_X,module='Bernoulli', mean=0, std=1,trials=None):\n",
    "        \"\"\"\n",
    "        Initializes the SampleSet with n samples and p features for X.\n",
    "        Y is generated based on the conditional probability P(Y=1|X).\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.p = p\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.r=None\n",
    "        self.B=None\n",
    "        self.module=module\n",
    "        # Generate X with dimension (n, p)\n",
    "        X_main = torch.normal(0.0, 1.0, size=(n, 2))\n",
    "        X_noise = torch.normal(0.0, 1, size=(n, p - 2))\n",
    "        self.X = torch.cat([X_main, X_noise], dim=1)\n",
    "\n",
    "        self.subtrain=None\n",
    "        self.subval=None\n",
    "        self.counts=None\n",
    "        self.trials=trials\n",
    "        self.f_X=f_X\n",
    "        # Compute z = f(X) and use it to generate P(Y=1|X) and Y\n",
    "        self.z = self._compute_z(self.X)  # Save z values (f(X))\n",
    "        self.Y = self._generate_Y(self.z)\n",
    "    \n",
    "    def _compute_z(self, X):\n",
    "        \"\"\"Computes f(X) = 1 - X_1 + X_2^2 + X_3 * X_4 and returns it.\"\"\"\n",
    "        return   self.f_X(X)\n",
    "    \n",
    "    def _generate_Y(self, z):\n",
    "        if self.module=='Bernoulli':\n",
    "            # Generate Y as a Bernoulli random variable with probability P(Y=1|X)\n",
    "            P_Y_given_X = 1 / (1 + torch.exp(-z))\n",
    "            Y = torch.bernoulli(P_Y_given_X)\n",
    "        elif self.module=='Gaussian':\n",
    "            Y = torch.normal(mean=z, std=1.0)\n",
    "        elif self.module=='Binomial':\n",
    "            if self.trials is None:\n",
    "                self.trials = torch.randint(low=5, high=6, size=(self.n,))  # Random n_i =5\n",
    "\n",
    "            P_Y_given_X = 1 / (1 + torch.exp(-z)) # Ensure the rate parameter is positive\n",
    "            Y = torch.binomial(self.trials.float(), P_Y_given_X)\n",
    "\n",
    "        elif self.module == 'Poisson':\n",
    "        # Generate Y as a Poisson random variable with rate parameter (lambda) equal to exp(z)\n",
    "            rate_param = torch.log(1+torch.exp(z))   # Ensure the rate parameter is positive\n",
    "            Y = torch.poisson(rate_param)\n",
    "        else:\n",
    "        # Raise an error for unsupported modules\n",
    "            raise ValueError(f\"Unsupported module type: {self.module}. Expected one of: 'Bernoulli', 'Binomial', 'Poisson'.\")\n",
    "        return Y\n",
    "    \n",
    "    def get_z(self):\n",
    "        \"\"\"Returns the computed z values, which represent f(X).\"\"\"\n",
    "        return self.z\n",
    "    \n",
    "    def get_sample_set(self):\n",
    "        \"\"\"Returns the main sample set (X, Y).\"\"\"\n",
    "        return self.X, self.Y\n",
    "    \n",
    "    def get_sub_samples_with_validation(self, B, r):\n",
    "        \"\"\"\n",
    "        Generates B sub-sample sets, each containing r samples randomly selected \n",
    "        from the main sample set, along with corresponding validation sets.\n",
    "        \n",
    "        Also counts the number of times each index is selected across all B sub-samples.\n",
    "        \n",
    "        Returns:\n",
    "            train_samples: List of tuples, each containing (train_X, train_Y, train_indices)\n",
    "            validation_samples: List of tuples, each containing (val_X, val_Y, val_indices)\n",
    "            selection_counts: Dictionary with counts of each index's appearance in the B sub-samples.\n",
    "        \"\"\"\n",
    "        train_samples = []\n",
    "        validation_samples = []\n",
    "        selection_counts = Counter({i: 0 for i in range(self.n)})   # To track appearances of each index\n",
    "        indices = torch.arange(self.n)\n",
    "        self.B=B\n",
    "        self.r=r\n",
    "\n",
    "        \n",
    "        for _ in range(B):\n",
    "            # Randomly select r unique indices for the sub-sample\n",
    "            selected_indices = indices[torch.randperm(self.n)[:r]]\n",
    "            \n",
    "            # Update selection count for each index\n",
    "            selection_counts.update(selected_indices.tolist())\n",
    "            \n",
    "            # Get validation indices (those not in selected_indices)\n",
    "            val_indices = torch.tensor([i for i in indices if i not in selected_indices])\n",
    "\n",
    "            # Separate sub-sample and validation sets, including original indices\n",
    "            X_sub = self.X[selected_indices]\n",
    "            Y_sub = self.Y[selected_indices]\n",
    "            \n",
    "            X_val = self.X[val_indices]\n",
    "            Y_val = self.Y[val_indices]\n",
    "            if self.module==\"Binomial\":\n",
    "                trial_sub=self.trials[selected_indices]\n",
    "                trial_val=self.trials[val_indices]\n",
    "            \n",
    "            # Append to train_samples and validation_samples lists\n",
    "                train_samples.append((X_sub, Y_sub, selected_indices,trial_sub))\n",
    "                validation_samples.append((X_val, Y_val, val_indices,trial_val))\n",
    "            else:\n",
    "                train_samples.append((X_sub, Y_sub, selected_indices))\n",
    "                validation_samples.append((X_val, Y_val, val_indices))\n",
    "        self.subtrain=train_samples\n",
    "        self.subval=validation_samples\n",
    "        self.counts=dict(selection_counts)\n",
    "        return train_samples, validation_samples, dict(selection_counts)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    def save(self, file_path):\n",
    "        \"\"\"Saves the SampleSet instance to a file.\"\"\"\n",
    "        torch.save(self, file_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        \"\"\"Loads a SampleSet instance from a file.\"\"\"\n",
    "        return torch.load(file_path)\n",
    "\n",
    "\n",
    "\n",
    "def clear_folder(folder_path):\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"folder {folder_path} does not exist\")\n",
    "        return\n",
    "\n",
    "\n",
    "    for item in os.listdir(folder_path):\n",
    "        item_path = os.path.join(folder_path, item)\n",
    "\n",
    "        if os.path.isfile(item_path):\n",
    "            os.remove(item_path)\n",
    "\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, dropout_rate=0.0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 10)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(10, 15)\n",
    "        self.bn2 = nn.BatchNorm1d(15)\n",
    "        self.layer3 = nn.Linear(15, 20)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.layer4 = nn.Linear(20, 30)\n",
    "        self.bn4 = nn.BatchNorm1d(30)\n",
    "        self.output = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.bn2(self.layer2(x)))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.bn4(self.layer4(x)))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "def train_network(network, train_loader,mode=\"Bernoulli\", num_epochs=100, learning_rate=0.1,weight_decay=0.05,tol=0.0001,patience=7):\n",
    "    if mode == \"Bernoulli\":\n",
    "        criterion = nn.BCEWithLogitsLoss() \n",
    "    elif mode==\"Poisson\":\n",
    "        criterion=poisson_loss\n",
    "    elif mode==\"Binomial\":\n",
    "        criterion=binomial_loss\n",
    "    else:\n",
    "    # Raise an error for unsupported modules\n",
    "        raise ValueError(f\"Unsupported module type {mode}. Expected one of: 'Bernoulli', 'Gaussian', 'Exponential', 'Poisson'.\")\n",
    "    \n",
    "    # BCELossWithLogits combines sigmoid and binary cross-entropy in one function\n",
    "    length=len(train_loader.dataset)\n",
    "    optimizer = optim.SGD(network.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "    prev_epoch_loss = None  \n",
    "    stable_count=0 \n",
    "    # Training loop\n",
    "    if mode==\"Binomial\":\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0  \n",
    "            for batch_X, batch_Y,batch_trial in train_loader:\n",
    "                # Forward pass\n",
    "                logits = network(batch_X).squeeze() # Get scalar logits, shape (batch_size)\n",
    "                # print(torch.max(logits))  \n",
    "                loss = criterion(logits, batch_Y.float(),batch_trial)  # Y needs to be float for BCELossWithLogits\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        # Compute average loss for the epoch\n",
    "            epoch_loss = running_loss / length\n",
    "            # print(f\"Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "            # Early‐stop check\n",
    "            if prev_epoch_loss is not None:\n",
    "                delta = abs(prev_epoch_loss - epoch_loss)\n",
    "                if delta < tol:\n",
    "                    stable_count += 1\n",
    "                else:\n",
    "                    stable_count = 0  # 重置计数\n",
    "\n",
    "                if stable_count >= patience:\n",
    "                    print(f\"Early stop at epoch {epoch} after {patience} stable epochs with loss {epoch_loss}.\")\n",
    "                    break\n",
    "            prev_epoch_loss = epoch_loss\n",
    "        print(f\"With loss {epoch_loss}.\")\n",
    "\n",
    "    else:\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0  \n",
    "            for batch_X, batch_Y in train_loader:\n",
    "                # Forward pass\n",
    "                logits = network(batch_X).squeeze() # Get scalar logits, shape (batch_size)\n",
    "                # print(torch.max(logits))  \n",
    "                loss = criterion(logits, batch_Y.float())  # Y needs to be float for BCELossWithLogits\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * batch_X.size(0)\n",
    "        \n",
    "        # Compute average loss for the epoch\n",
    "            epoch_loss = running_loss / length\n",
    "            # print(f\"Epoch {epoch}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "            # Early‐stop check\n",
    "            if prev_epoch_loss is not None:\n",
    "                delta = abs(prev_epoch_loss - epoch_loss)\n",
    "                if delta < tol:\n",
    "                    stable_count += 1\n",
    "                else:\n",
    "                    stable_count = 0  # 重置计数\n",
    "\n",
    "                if stable_count >= patience:\n",
    "                    print(f\"Early stop at epoch {epoch} after {patience} stable epochs with loss {epoch_loss}.\")\n",
    "                    break\n",
    "            prev_epoch_loss = epoch_loss\n",
    "        print(f\"With loss {epoch_loss}.\")\n",
    "\n",
    "# Instantiate and train B neural networks, one for each sub-sample\n",
    "def train_multiple_networks(sample_set, input_size, mode=\"Bernoulli\",batchsize=64, dropout_rate=0.1, num_epochs=100, learning_rate=0.01,weight_decay=0.05,tol=0.0001,patience=7):\n",
    "    train_samples, validation_samples, selection_counts = sample_set.subtrain, sample_set.subval, sample_set.counts\n",
    "    \n",
    "    networks = []  # List to hold trained neural networks\n",
    "    for i, (train_data, val_data) in enumerate(zip(train_samples, validation_samples)):\n",
    "\n",
    "        if sample_set.module==\"Binomial\":\n",
    "            X_sub, Y_sub, _,trial_sub = train_data\n",
    "            X_val, Y_val, _,trial_val = val_data\n",
    "\n",
    "            \n",
    "            # Prepare data loader for this sub-sample\n",
    "            train_dataset = TensorDataset(X_sub, Y_sub,trial_sub)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "  \n",
    "        else:\n",
    "            X_sub, Y_sub, _ = train_data\n",
    "            X_val, Y_val, _ = val_data\n",
    "\n",
    "            \n",
    "            # Prepare data loader for this sub-sample\n",
    "            train_dataset = TensorDataset(X_sub, Y_sub)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
    "            \n",
    "        # Instantiate a new network for this sub-sample\n",
    "        network = NeuralNetwork(input_size,  dropout_rate=dropout_rate)\n",
    "        # print(f\"\\nTraining network {i+1}/{B} on sub-sample {i+1}\")\n",
    "        \n",
    "        # Train the network on the current sub-sample\n",
    "        train_network(network, train_loader,mode=sample_set.module, num_epochs=num_epochs, learning_rate=learning_rate,weight_decay=weight_decay)\n",
    "        \n",
    "        # Append the trained network to the list of networks\n",
    "        networks.append(network)\n",
    "\n",
    "        # Validation performance (optional)\n",
    "        with torch.no_grad():\n",
    "            if sample_set.module==\"Bernoulli\":\n",
    "                logits = network(X_val).squeeze()\n",
    "                true_logits=f_1(X_val)\n",
    "                accuracy = (torch.sign(logits) == torch.sign(true_logits)).float().mean() * 100\n",
    "                print(f\"Validation Accuracy for network {i+1}: {accuracy.item():.2f}%\")\n",
    "                print(torch.mean(abs(logits-true_logits)))\n",
    "            elif sample_set.module == \"Poisson\":\n",
    "                logits = network(X_val).squeeze()\n",
    "                # print(torch.max(logits))\n",
    "                true_lambda=torch.log(1+torch.exp(f_1(X_val)))\n",
    "                estimated_lambda=torch.exp(logits)\n",
    "                print(f\" network {i+1}: {torch.max(true_lambda),torch.min(true_lambda)}%\")\n",
    "                # print(torch.max(estimated_lambda))\n",
    "                # print(torch.max(true_lambda-estimated_lambda),torch.min(true_lambda-estimated_lambda))\n",
    "                print(torch.mean(abs(true_lambda-estimated_lambda)),torch.std(abs(true_lambda-estimated_lambda)))\n",
    "                # print(torch.mean(estimated_lambda))\n",
    "            elif sample_set.module == \"Binomial\":\n",
    "                logits = network(X_val).squeeze()\n",
    "                true_logits=f_1(X_val)\n",
    "                print(torch.mean(abs(torch.sigmoid(logits)-torch.sigmoid(true_logits))))\n",
    "                # print(torch.max(abs(torch.sigmoid(logits)-torch.sigmoid(true_logits))))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported module type {sample_set.module}. Expected one of: 'Bernoulli', 'Gaussian', 'Exponential', 'Poisson'.\")\n",
    "\n",
    "\n",
    "    \n",
    "    return networks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ensemble_predict_batch_f(Xtest, networks, sample_set):\n",
    "    ntest = Xtest.shape[0]\n",
    "    n = sample_set.n       # Total number of original samples\n",
    "    r = sample_set.r       # Size of each sub-sample\n",
    "    B = len(networks)      # Number of sub-samples (number of neural networks)\n",
    "\n",
    "    # Collect logits from all networks for the test set (shape: [ntest, B])\n",
    "    all_outputs = torch.zeros(ntest, B)\n",
    "    with torch.no_grad():\n",
    "        for j, net in enumerate(networks):\n",
    "            logits = net(Xtest).squeeze(1)\n",
    "            all_outputs[:, j] = logits\n",
    "\n",
    "    # Compute inclusion counts J_bji and mean inclusion J_dot_i for each training index i\n",
    "    J_bji = sample_set.counts  # Dict mapping i -> count of i in each sub-sample\n",
    "    J_dot_i = {i: J_bji[i] / B for i in range(n)}\n",
    "\n",
    "    # Ensemble mean prediction for each test sample\n",
    "    hatf_B = all_outputs.mean(dim=1)  # Shape: [ntest]\n",
    "\n",
    "    # Initialize accumulators for variance correction terms\n",
    "    sum_V2 = torch.zeros(ntest)      # Accumulate sum of hat_V_i^2 over i\n",
    "    sum_Zdiff2 = torch.zeros(ntest)  # Accumulate sum of (Z_ji - hat_V_i)^2 over i and j\n",
    "    if sample_set.module=='Binomial':\n",
    "        for i in range(n):\n",
    "            # Gather Z_{b_j i}(x*) for all sub-samples j (shape: [B, ntest])\n",
    "            Zs = torch.zeros(B, ntest)\n",
    "            for j in range(B):\n",
    "                _, _, Jbjicount,_ = sample_set.subtrain[j]\n",
    "                in_subset = 1.0 if (i in Jbjicount) else 0.0\n",
    "                deviations = all_outputs[:, j] - hatf_B  # Shape: [ntest]\n",
    "                Zs[j] = (in_subset - J_dot_i[i]) * deviations\n",
    "\n",
    "            # Compute hat_V_i(x*) and accumulate\n",
    "            hat_V_i = Zs.mean(dim=0)  # Shape: [ntest]\n",
    "            sum_V2 += hat_V_i.pow(2)\n",
    "            sum_Zdiff2 += (Zs - hat_V_i.unsqueeze(0)).pow(2).sum(dim=0)\n",
    "\n",
    "\n",
    "    else: \n",
    "        # Loop over each original data index i\n",
    "        for i in range(n):\n",
    "            # Gather Z_{b_j i}(x*) for all sub-samples j (shape: [B, ntest])\n",
    "            Zs = torch.zeros(B, ntest)\n",
    "            for j in range(B):\n",
    "                _, _, Jbjicount = sample_set.subtrain[j]\n",
    "                in_subset = 1.0 if (i in Jbjicount) else 0.0\n",
    "                deviations = all_outputs[:, j] - hatf_B  # Shape: [ntest]\n",
    "                Zs[j] = (in_subset - J_dot_i[i]) * deviations\n",
    "\n",
    "            # Compute hat_V_i(x*) and accumulate\n",
    "            hat_V_i = Zs.mean(dim=0)  # Shape: [ntest]\n",
    "            sum_V2 += hat_V_i.pow(2)\n",
    "            sum_Zdiff2 += (Zs - hat_V_i.unsqueeze(0)).pow(2).sum(dim=0)\n",
    "\n",
    "    # Correction factor: n(n-1)/(n-r)^2\n",
    "    factor = (n - 1) / n * (n / (n - r))**2\n",
    "\n",
    "    # Compute corrected variance terms\n",
    "    term1 = factor * sum_V2\n",
    "    term2 = factor * sum_Zdiff2 / (B * (B - 1))\n",
    "    var_f = term1 - term2          # Bias-corrected variance estimate\n",
    "\n",
    "    # Standard deviations\n",
    "    sd_f_raw = torch.sqrt(term1)       # Without bias correction\n",
    "    sd_f_correct = torch.sqrt(var_f)   # With bias correction\n",
    "\n",
    "\n",
    "    return all_outputs, [hatf_B, sd_f_raw, sd_f_correct]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_one_repeat(rep_id, n, r, B, p, GLM_name, f_1, xtest):\n",
    "    # 每个 repeat 训练 B 个网络，最后做一次 ensemble 预测\n",
    "    ss = SampleSet(n, p, f_1, module=GLM_name)\n",
    "    ss.get_sub_samples_with_validation(B, r)\n",
    "\n",
    "    networks = train_multiple_networks(\n",
    "        ss,\n",
    "        input_size=p,\n",
    "        mode=GLM_name,\n",
    "        batchsize=r,\n",
    "        num_epochs=500,\n",
    "        learning_rate=0.1,\n",
    "        weight_decay=0.02,\n",
    "        dropout_rate=0.1,\n",
    "        tol=0.0001,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    _, Bf = ensemble_predict_batch_f(xtest, networks, ss)\n",
    "    return Bf[0], Bf[1],Bf[2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 400\n",
    "p=10 # Main sample size\n",
    "B = 1400   \n",
    "GLM_name='Poisson' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With loss 0.5079498291015625.\n",
      "biasf: tensor(-0.3823, grad_fn=<MeanBackward0>) biasfsd: tensor(1.3055, grad_fn=<StdBackward0>)\n",
      "MAEf: tensor(0.9693, grad_fn=<MeanBackward0>) MAEfsd: tensor(0.9492, grad_fn=<StdBackward0>)\n",
      "biaspsi: tensor(-0.0003, grad_fn=<MeanBackward0>) biaspsisd: tensor(0.8312, grad_fn=<StdBackward0>)\n",
      "MAEpsi: tensor(0.5904, grad_fn=<MeanBackward0>) MAEpsisd: tensor(0.5813, grad_fn=<StdBackward0>)\n",
      "EmpSD: tensor(1.3525, grad_fn=<SqrtBackward0>)\n",
      "tensor(3.1146, grad_fn=<SqueezeBackward4>)\n",
      "tensor(93.7500)\n",
      "AIL: tensor(20.8727, grad_fn=<MeanBackward0>)\n",
      "sd: tensor(16.9415, grad_fn=<StdBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3738173/2760695594.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions_logits_train2 = network1(torch.tensor(X_sub2)).squeeze()  # Model predictions on training data\n",
      "/tmp/ipykernel_3738173/2760695594.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_logits2=torch.log(torch.log(1+torch.exp(torch.tensor(f(X_sub2)))))\n",
      "/tmp/ipykernel_3738173/2760695594.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
      "/tmp/ipykernel_3738173/2760695594.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_logits_test=torch.log(torch.log(1+torch.exp(torch.tensor(f(xtest)))))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "function_name='f_1'\n",
    "r=int(n**(0.9))\n",
    "input_size = p\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "\n",
    "torch.manual_seed(126)\n",
    "filename_data=f'{GLM_name}_{function_name}n{n}p{p}B{B}r{r}.pth'\n",
    "filename_model=f'{GLM_name}_{function_name}n{n}p{p}B{B}r{r}h1{hidden_size1}h2{hidden_size2}'\n",
    "sample_set = SampleSet(n, p, f_1, module=GLM_name)\n",
    "\n",
    "\n",
    "\n",
    "X_sub,Y_sub=sample_set.get_sample_set()\n",
    "X_sub1,Y_sub1=X_sub[0:int(n/2),],Y_sub[0:int(n/2)]\n",
    "X_sub2,Y_sub2=X_sub[int(n/2):n,],Y_sub[int(n/2):n]\n",
    "\n",
    "if GLM_name==\"Binomial\":\n",
    "    trials_sub=sample_set.trials\n",
    "    X_trial1=trials_sub[int(n/2):n]\n",
    "    X_trial2=trials_sub[int(n/2):n,]\n",
    "    train_dataset1 = TensorDataset(X_sub1, Y_sub1,X_trial1)\n",
    "\n",
    "else: \n",
    "    train_dataset1 = TensorDataset(X_sub1, Y_sub1)\n",
    "\n",
    "\n",
    "\n",
    "train_loader1 = DataLoader(train_dataset1, batch_size=int(n/2), shuffle=True)\n",
    "\n",
    "\n",
    "network1 = NeuralNetwork(input_size, dropout_rate=0.1)\n",
    "\n",
    "# Train the network on the current sub-sample\n",
    "train_network(network1, train_loader1,mode=GLM_name, num_epochs=500, learning_rate=0.1,weight_decay=0.02)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "if function_name=='f_1':\n",
    "    f=f_1\n",
    "else:\n",
    "    f=f_2\n",
    "    \n",
    "\n",
    "if GLM_name==\"Bernoulli\":\n",
    "\n",
    "\n",
    "    predictions_logits_train2 = network1(torch.tensor(X_sub2)).squeeze()  # Model predictions on training data\n",
    "    true_logits2=torch.tensor(f_1(X_sub2))\n",
    "    predictions_train2=1/(1+torch.exp(-predictions_logits_train2))\n",
    "    true_f2=1/(1+torch.exp(-true_logits2))\n",
    "    residuals2 = torch.abs(true_logits2 - predictions_logits_train2)  # Absolute residuals\n",
    "\n",
    "    # quantile=torch.quantile(torch.cat((residuals1,residuals2),dim=0),0.95)\n",
    "\n",
    "    torch.manual_seed(23)\n",
    "    xtest=torch.normal(0, 1, (80,p))\n",
    "    quantile = torch.quantile(residuals2, (1 - 0.05))\n",
    "\n",
    "\n",
    "    predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
    "    # predictions_logits_test=(network2(torch.tensor(xtest)).squeeze()+network2(torch.tensor(xtest)).squeeze())/2\n",
    "    true_logits_test=torch.tensor(f_1(xtest))\n",
    "    true_f_test=1/(1+torch.exp(-true_logits_test))\n",
    "    predictions_test=1/(1+torch.exp(-predictions_logits_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"biasf:\",torch.mean(predictions_logits_test-true_logits_test),\"biasfsd:\",torch.std(predictions_logits_test-true_logits_test))    \n",
    "    print(\"MAEf:\",torch.mean(abs(predictions_logits_test-true_logits_test)),\"MAEfsd:\",torch.std(abs(predictions_logits_test-true_logits_test)))    \n",
    "\n",
    "    print(\"biaspsi:\",torch.mean(predictions_test-true_f_test),\"biaspsisd:\",torch.std(predictions_test-true_f_test))\n",
    "    print('MAEpsi:',(torch.mean(abs(true_f_test-predictions_test))),'MAEpsisd:',(torch.std(abs(true_f_test-predictions_test))))\n",
    "    print(\"EpmSD:\",torch.sqrt(torch.sum((predictions_logits_test-true_logits_test)**2)/len(predictions_logits_test)))\n",
    "\n",
    "    within_confidence_interval = ((true_logits_test >= predictions_logits_test-quantile) & (true_logits_test <= predictions_logits_test+quantile))\n",
    "\n",
    "    # within_confidence_interval = ((sigmoid(true_logits_test) >= sigmoid(predictions_logits_test)-quantile) & (sigmoid(true_logits_test) <= sigmoid(predictions_logits_test)+quantile))\n",
    "\n",
    "    # Step 2: Count the number of times true_probability is within the confidence interval\n",
    "    count_within_interval = within_confidence_interval.sum()\n",
    "\n",
    "    # Step 3: Calculate the frequency\n",
    "    frequency_within_interval = count_within_interval / len(predictions_logits_test) * 100  # as a percentage\n",
    "    print(frequency_within_interval)\n",
    "    print(quantile)\n",
    "    CI_length=torch.mean(abs(1/(1+torch.exp(-predictions_logits_test+quantile))-1/(1+torch.exp(-predictions_logits_test-quantile))))\n",
    "    print(\"AIC:\",CI_length)\n",
    "    CI_length_sd=torch.std(abs(1/(1+torch.exp(-predictions_logits_test+quantile))-1/(1+torch.exp(-predictions_logits_test-quantile))))\n",
    "    print(\"sd:\",CI_length_sd)\n",
    "\n",
    "elif GLM_name==\"Poisson\":\n",
    "    predictions_logits_train2 = network1(torch.tensor(X_sub2)).squeeze()  # Model predictions on training data\n",
    "    true_logits2=torch.log(torch.log(1+torch.exp(torch.tensor(f(X_sub2)))))\n",
    "    predictions_lambda2=torch.exp(predictions_logits_train2)\n",
    "    true_lambda2=torch.exp(true_logits2)\n",
    "    residuals2 = torch.abs(true_logits2 - predictions_logits_train2)  # Absolute residuals\n",
    "\n",
    "    quantile = torch.quantile(residuals2, (1 - 0.05))\n",
    "\n",
    "\n",
    "    xtest=torch.normal(0, 1, (80,p))\n",
    "\n",
    "\n",
    "    predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
    "    true_logits_test=torch.log(torch.log(1+torch.exp(torch.tensor(f(xtest)))))\n",
    "\n",
    "    true_lambda_test=torch.exp(true_logits_test)\n",
    "    predictions_lambda_test=torch.exp(predictions_logits_test)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"biasf:\",torch.mean(predictions_logits_test-true_logits_test),\"biasfsd:\",torch.std(predictions_logits_test-true_logits_test))\n",
    "    print(\"MAEf:\",torch.mean(abs(predictions_logits_test-true_logits_test)),\"MAEfsd:\",torch.std(abs(predictions_logits_test-true_logits_test)))\n",
    "    print(\"biaspsi:\",torch.mean(predictions_lambda_test-true_lambda_test),\"biaspsisd:\",torch.std(predictions_lambda_test-true_lambda_test))\n",
    "    print('MAEpsi:',torch.mean(abs(true_lambda_test-predictions_lambda_test)),'MAEpsisd:',torch.std(abs(true_lambda_test-predictions_lambda_test)))\n",
    "    print(\"EmpSD:\",torch.sqrt(torch.sum((predictions_logits_test-true_logits_test)**2)/len(predictions_logits_test)))\n",
    "\n",
    "    # print(torch.max(predictions_logits_test))\n",
    "\n",
    "    # print(predictions_lambda_test[1:60])\n",
    "    # print(max(predictions_lambda_test+quantile))\n",
    "    # print(max(true_lambda_test))\n",
    "    print(quantile)\n",
    "\n",
    "    within_confidence_interval = ((true_logits_test >= predictions_logits_test-quantile) & (true_logits_test <= predictions_logits_test+quantile))\n",
    "\n",
    "    # Step 2: Count the number of times true_probability is within the confidence interval\n",
    "    count_within_interval = within_confidence_interval.sum()\n",
    "\n",
    "    # Step 3: Calculate the frequency\n",
    "    frequency_within_interval = count_within_interval / len(predictions_logits_test) * 100  # as a percentage\n",
    "    print(frequency_within_interval)\n",
    "\n",
    "    CI_length=torch.mean(abs(torch.exp(predictions_logits_test+quantile)-torch.exp(predictions_logits_test-quantile)))\n",
    "    print(\"AIL:\",CI_length)\n",
    "    CI_length_sd=torch.std(abs(torch.exp(predictions_logits_test+quantile)-torch.exp(predictions_logits_test-quantile)))\n",
    "    print(\"sd:\",CI_length_sd)\n",
    "else: \n",
    "\n",
    "    predictions_logits_train2 = network1(torch.tensor(X_sub2)).squeeze()  # Model predictions on training data\n",
    "    true_logits2=torch.tensor(f_1(X_sub2))\n",
    "    predictions_train2=1/(1+torch.exp(-predictions_logits_train2))\n",
    "    true_f2=1/(1+torch.exp(-true_logits2))\n",
    "    residuals2 = torch.abs(true_logits2 - predictions_logits_train2)  # Absolute residuals\n",
    "\n",
    "    # quantile=torch.quantile(torch.cat((residuals1,residuals2),dim=0),0.95)\n",
    "\n",
    "    torch.manual_seed(23)\n",
    "    xtest=torch.normal(0, 1, (80,p))\n",
    "    quantile = torch.quantile(residuals2, (1 - 0.05))\n",
    "\n",
    "\n",
    "    predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
    "    # predictions_logits_test=(network2(torch.tensor(xtest)).squeeze()+network2(torch.tensor(xtest)).squeeze())/2\n",
    "    true_logits_test=torch.tensor(f_1(xtest))\n",
    "    true_f_test=1/(1+torch.exp(-true_logits_test))\n",
    "    predictions_test=1/(1+torch.exp(-predictions_logits_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"biasf:\",torch.mean(predictions_logits_test-true_logits_test))    \n",
    "    print(\"MAEf:\",torch.mean(abs(predictions_logits_test-true_logits_test)))    \n",
    "    print(\"MAEfsd:\",torch.std(abs(predictions_logits_test-true_logits_test)))   \n",
    "    print(\"biaspsi:\",torch.mean(predictions_test-true_f_test))\n",
    "    print('MAEpsi:',(torch.mean(abs(true_f_test-predictions_test))))\n",
    "    print('MAEpsisd:',(torch.std(abs(true_f_test-predictions_test))))\n",
    "    print(\"EpmSD:\",torch.sqrt(torch.sum((predictions_logits_test-true_logits_test)**2)/len(predictions_logits_test)))\n",
    "\n",
    "    within_confidence_interval = ((true_logits_test >= predictions_logits_test-quantile) & (true_logits_test <= predictions_logits_test+quantile))\n",
    "\n",
    "    # within_confidence_interval = ((sigmoid(true_logits_test) >= sigmoid(predictions_logits_test)-quantile) & (sigmoid(true_logits_test) <= sigmoid(predictions_logits_test)+quantile))\n",
    "\n",
    "    # Step 2: Count the number of times true_probability is within the confidence interval\n",
    "    count_within_interval = within_confidence_interval.sum()\n",
    "\n",
    "    # Step 3: Calculate the frequency\n",
    "    frequency_within_interval = count_within_interval / len(predictions_logits_test) * 100  # as a percentage\n",
    "    print(frequency_within_interval)\n",
    "    print(quantile)\n",
    "    CI_length=torch.mean(abs(1/(1+torch.exp(-predictions_logits_test+quantile))-1/(1+torch.exp(-predictions_logits_test-quantile))))\n",
    "    print(\"AIC:\",CI_length)\n",
    "    CI_length_sd=torch.std(abs(1/(1+torch.exp(-predictions_logits_test+quantile))-1/(1+torch.exp(-predictions_logits_test-quantile))))\n",
    "    print(\"sd:\",CI_length_sd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "With loss 0.2975127398967743.\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3738173/2745128317.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions_logits_train2 = network1(torch.tensor(X_sub2)).clone().detach().squeeze()  # Model predictions on training data\n",
      "/tmp/ipykernel_3738173/2745128317.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_logits2=torch.tensor(f_1(X_sub2))\n",
      "/tmp/ipykernel_3738173/2745128317.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With loss 0.3306395709514618.\n",
      "2\n",
      "With loss 0.3748362064361572.\n",
      "3\n",
      "With loss 0.30596643686294556.\n",
      "4\n",
      "With loss 0.35511329770088196.\n",
      "5\n",
      "With loss 0.27200281620025635.\n",
      "6\n",
      "With loss 0.3251241147518158.\n",
      "7\n",
      "With loss 0.29820695519447327.\n",
      "8\n",
      "With loss 0.33640316128730774.\n",
      "9\n",
      "With loss 0.40474221110343933.\n",
      "10\n",
      "With loss 0.3220440149307251.\n",
      "11\n",
      "With loss 0.36167341470718384.\n",
      "12\n",
      "With loss 0.3439752161502838.\n",
      "13\n",
      "With loss 0.3581956624984741.\n",
      "14\n",
      "With loss 0.35045912861824036.\n",
      "15\n",
      "With loss 0.31318238377571106.\n",
      "16\n",
      "With loss 0.34927910566329956.\n",
      "17\n",
      "With loss 0.4166376292705536.\n",
      "18\n",
      "With loss 0.43191924691200256.\n",
      "19\n",
      "With loss 0.4177580773830414.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "function_name='f_1'\n",
    "r=int(n**(0.9))\n",
    "input_size = p\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "\n",
    "torch.manual_seed(13)\n",
    "filename_data=f'{GLM_name}_{function_name}n{n}p{p}B{B}r{r}.pth'\n",
    "filename_model=f'{GLM_name}_{function_name}n{n}p{p}B{B}r{r}h1{hidden_size1}h2{hidden_size2}'\n",
    "xtest    = torch.load(f\"xtest10.pt\")\n",
    "prediction_logit_list=torch.zeros(20, 80)\n",
    "quantile_list=torch.zeros(20)\n",
    "for repeat0 in range(20): \n",
    "    print(repeat0)\n",
    "    sample_set = SampleSet(n, p, f_1, module=GLM_name)\n",
    "\n",
    "\n",
    "\n",
    "    X_sub,Y_sub=sample_set.get_sample_set()\n",
    "    X_sub1,Y_sub1=X_sub[0:int(n/2),],Y_sub[0:int(n/2)]\n",
    "    X_sub2,Y_sub2=X_sub[int(n/2):n,],Y_sub[int(n/2):n]\n",
    "\n",
    "    if GLM_name==\"Binomial\":\n",
    "        trials_sub=sample_set.trials\n",
    "        X_trial1=trials_sub[int(n/2):n]\n",
    "        X_trial2=trials_sub[int(n/2):n,]\n",
    "        train_dataset1 = TensorDataset(X_sub1, Y_sub1,X_trial1)\n",
    "\n",
    "    else: \n",
    "        train_dataset1 = TensorDataset(X_sub1, Y_sub1)\n",
    "\n",
    "\n",
    "\n",
    "    train_loader1 = DataLoader(train_dataset1, batch_size=int(n/2), shuffle=True)\n",
    "\n",
    "\n",
    "    network1 = NeuralNetwork(input_size,  dropout_rate=0.1)\n",
    "\n",
    "    # Train the network on the current sub-sample\n",
    "    train_network(network1, train_loader1,mode=GLM_name, num_epochs=500, learning_rate=0.1,weight_decay=0.02)\n",
    "\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+torch.exp(-x))\n",
    "\n",
    "    if function_name=='f_1':\n",
    "        f=f_1\n",
    "    else:\n",
    "        f=f_2\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if GLM_name==\"Bernoulli\":\n",
    "\n",
    "\n",
    "        predictions_logits_train2 = network1(torch.tensor(X_sub2)).clone().detach().squeeze()  # Model predictions on training data\n",
    "        true_logits2=torch.tensor(f_1(X_sub2))\n",
    "        predictions_train2=1/(1+torch.exp(-predictions_logits_train2))\n",
    "        true_f2=1/(1+torch.exp(-true_logits2))\n",
    "        residuals2 = torch.abs(true_logits2 - predictions_logits_train2)  # Absolute residuals\n",
    "\n",
    "        # quantile=torch.quantile(torch.cat((residuals1,residuals2),dim=0),0.95)\n",
    "\n",
    "        \n",
    "        \n",
    "        quantile = torch.quantile(residuals2, (1 - 0.05))\n",
    "\n",
    "\n",
    "        predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
    "        prediction_logit_list[repeat0,:]=predictions_logits_test\n",
    "        quantile_list[repeat0]=quantile\n",
    "        # predictions_logits_test=(network2(torch.tensor(xtest)).squeeze()+network2(torch.tensor(xtest)).squeeze())/2\n",
    "        # true_logits_test=torch.tensor(f_1(xtest))\n",
    "        # true_f_test=1/(1+torch.exp(-true_logits_test))\n",
    "        # predictions_test=1/(1+torch.exp(-predictions_logits_test))\n",
    "\n",
    "\n",
    "    elif GLM_name==\"Poisson\":\n",
    "        predictions_logits_train2 = network1(torch.tensor(X_sub2)).clone().detach().squeeze()  # Model predictions on training data\n",
    "        true_logits2=torch.log(torch.log(1+torch.exp(torch.tensor(f(X_sub2)))))\n",
    "        predictions_lambda2=torch.exp(predictions_logits_train2)\n",
    "        true_lambda2=torch.exp(true_logits2)\n",
    "        residuals2 = torch.abs(true_logits2 - predictions_logits_train2)  # Absolute residuals\n",
    "\n",
    "        quantile = torch.quantile(residuals2, (1 - 0.05))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
    "        prediction_logit_list[repeat0,:]=predictions_logits_test\n",
    "        quantile_list[repeat0]=quantile\n",
    "\n",
    "\n",
    "\n",
    "    else: \n",
    "\n",
    "        predictions_logits_train2 = network1(torch.tensor(X_sub2)).squeeze()  # Model predictions on training data\n",
    "        true_logits2=torch.tensor(f_1(X_sub2))\n",
    "        predictions_train2=1/(1+torch.exp(-predictions_logits_train2))\n",
    "        true_f2=1/(1+torch.exp(-true_logits2))\n",
    "        residuals2 = torch.abs(true_logits2 - predictions_logits_train2)  # Absolute residuals\n",
    "\n",
    "        # quantile=torch.quantile(torch.cat((residuals1,residuals2),dim=0),0.95)\n",
    "\n",
    "        \n",
    "        quantile = torch.quantile(residuals2, (1 - 0.05))\n",
    "\n",
    "\n",
    "        predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
    "        prediction_logit_list[repeat0,:]=predictions_logits_test\n",
    "        quantile_list[repeat0]=quantile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_matrix = quantile_list.view(-1, 1)  \n",
    "quantile_matrix = quantile_matrix.expand_as(prediction_logit_list) \n",
    "\n",
    "true_matrix = true_logits_test.unsqueeze(0).expand_as(prediction_logit_list)  # [20, 80]\n",
    "quantile_matrix = quantile_list.view(-1, 1).expand_as(prediction_logit_list)  # [20, 80]\n",
    "\n",
    "# Confidence interval bounds\n",
    "lower_bound = prediction_logit_list - quantile_matrix\n",
    "upper_bound = prediction_logit_list + quantile_matrix\n",
    "\n",
    "# --------------------------\n",
    "# Biasf and MAEf\n",
    "mean_prediction = prediction_logit_list.mean(dim=0)  # [80]\n",
    "bias_f = (mean_prediction - true_logits_test).mean().item()\n",
    "mae_f = (mean_prediction - true_logits_test).abs().mean().item()\n",
    "\n",
    "# --------------------------\n",
    "# ψ' transformation: assume logistic model, so ψ'(x) = sigmoid(x)\n",
    "true_psi = torch.sigmoid(true_logits_test)  # [80]\n",
    "pred_psi = torch.sigmoid(mean_prediction)   # [80]\n",
    "\n",
    "bias_psi = (pred_psi - true_psi).mean().item()\n",
    "mae_psi = (pred_psi - true_psi).abs().mean().item()\n",
    "\n",
    "# --------------------------\n",
    "# Empirical Standard Deviation\n",
    "emp_sd = prediction_logit_list.std(dim=0)  # [80]\n",
    "emp_sd_mean = emp_sd.mean().item()\n",
    "\n",
    "# --------------------------\n",
    "# Coverage Probability\n",
    "covered = (true_matrix >= lower_bound) & (true_matrix <= upper_bound)  # [20, 80]\n",
    "cp = covered.float().mean(dim=0).mean().item()  # scalar: average over 80 points\n",
    "\n",
    "# --------------------------\n",
    "# AIL (average interval length) after sigmoid\n",
    "upper_sigmoid = torch.sigmoid(upper_bound)\n",
    "lower_sigmoid = torch.sigmoid(lower_bound)\n",
    "ail = (upper_sigmoid - lower_sigmoid).mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787343859672546"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biasf: tensor(-0.1020, grad_fn=<MeanBackward0>) biasf: tensor(2.0035, grad_fn=<StdBackward0>)\n",
      "MAEf: tensor(1.7192, grad_fn=<MeanBackward0>) MAEfsd: tensor(1.0157, grad_fn=<StdBackward0>)\n",
      "biaspsi: tensor(-0.0352, grad_fn=<MeanBackward0>) biaspsisd: tensor(0.2792, grad_fn=<StdBackward0>)\n",
      "MAEpsi: tensor(0.2489, grad_fn=<MeanBackward0>) MAEpsisd: tensor(0.1284, grad_fn=<StdBackward0>)\n",
      "EpmSD: tensor(1.9936, grad_fn=<SqrtBackward0>)\n",
      "Coverp: tensor(88.7500)\n",
      "AIC: tensor(0.6732, grad_fn=<MeanBackward0>)\n",
      "sd: tensor(0.2170, grad_fn=<StdBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3738173/2844922002.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions_logits_train2 = network1(torch.tensor(X_sub2)).squeeze()  # Model predictions on training data\n",
      "/tmp/ipykernel_3738173/2844922002.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_logits2=torch.tensor(f_1(X_sub2))\n",
      "/tmp/ipykernel_3738173/2844922002.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
      "/tmp/ipykernel_3738173/2844922002.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  true_logits_test=torch.tensor(f_1(xtest))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "if GLM_name==\"Bernoulli\":\n",
    "\n",
    "\n",
    "    predictions_logits_train2 = network1(torch.tensor(X_sub2)).squeeze()  # Model predictions on training data\n",
    "    true_logits2=torch.tensor(f_1(X_sub2))\n",
    "    predictions_train2=1/(1+torch.exp(-predictions_logits_train2))\n",
    "    true_f2=1/(1+torch.exp(-true_logits2))\n",
    "    residuals2 = torch.abs(true_logits2 - predictions_logits_train2)  # Absolute residuals\n",
    "\n",
    "    # quantile=torch.quantile(torch.cat((residuals1,residuals2),dim=0),0.95)\n",
    "\n",
    "    \n",
    "    \n",
    "    quantile = torch.quantile(residuals2, (1 - 0.05))\n",
    "\n",
    "\n",
    "    predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
    "    # predictions_logits_test=(network2(torch.tensor(xtest)).squeeze()+network2(torch.tensor(xtest)).squeeze())/2\n",
    "    true_logits_test=torch.tensor(f_1(xtest))\n",
    "    true_f_test=1/(1+torch.exp(-true_logits_test))\n",
    "    predictions_test=1/(1+torch.exp(-predictions_logits_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"biasf:\",torch.mean(predictions_logits_test-true_logits_test),\"biasf:\",torch.std(predictions_logits_test-true_logits_test))    \n",
    "    print(\"MAEf:\",torch.mean(abs(predictions_logits_test-true_logits_test)),\"MAEfsd:\",torch.std(abs(predictions_logits_test-true_logits_test)))     \n",
    "    print(\"biaspsi:\",torch.mean(predictions_test-true_f_test),\"biaspsisd:\",torch.std(predictions_test-true_f_test))\n",
    "    print('MAEpsi:',(torch.mean(abs(true_f_test-predictions_test))),'MAEpsisd:',(torch.std(abs(true_f_test-predictions_test))))\n",
    "    # print('MAEpsisd:',(torch.std(abs(true_f_test-predictions_test))))\n",
    "    print(\"EpmSD:\",torch.sqrt(torch.sum((predictions_logits_test-true_logits_test)**2)/len(predictions_logits_test)))\n",
    "\n",
    "    within_confidence_interval = ((true_logits_test >= predictions_logits_test-quantile) & (true_logits_test <= predictions_logits_test+quantile))\n",
    "\n",
    "    # within_confidence_interval = ((sigmoid(true_logits_test) >= sigmoid(predictions_logits_test)-quantile) & (sigmoid(true_logits_test) <= sigmoid(predictions_logits_test)+quantile))\n",
    "\n",
    "    # Step 2: Count the number of times true_probability is within the confidence interval\n",
    "    count_within_interval = within_confidence_interval.sum()\n",
    "\n",
    "    # Step 3: Calculate the frequency\n",
    "    frequency_within_interval = count_within_interval / len(predictions_logits_test) * 100  # as a percentage\n",
    "    print(\"Coverp:\",frequency_within_interval)\n",
    "    CI_length=torch.mean(abs(1/(1+torch.exp(-predictions_logits_test+quantile))-1/(1+torch.exp(-predictions_logits_test-quantile))))\n",
    "    print(\"AIC:\",CI_length)\n",
    "    CI_length_sd=torch.std(abs(1/(1+torch.exp(-predictions_logits_test+quantile))-1/(1+torch.exp(-predictions_logits_test-quantile))))\n",
    "    print(\"sd:\",CI_length_sd)\n",
    "\n",
    "elif GLM_name==\"Poisson\":\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    true_logits_test=torch.log(torch.log(1+torch.exp(torch.tensor(f(xtest)))))\n",
    "    \n",
    "    true_lambda_test=torch.exp(true_logits_test)\n",
    "    predictions_lambda_test=torch.exp(prediction_logit_list)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"biasf:\",torch.mean(prediction_logit_list-true_logits_test),\"biasfsd:\",torch.std(prediction_logit_list-true_logits_test))\n",
    "    print(\"biasf:\",torch.mean(abs(prediction_logit_list-true_logits_test)),\"biasfsd:\",torch.std(abs(prediction_logit_list-true_logits_test)))\n",
    "    print(\"biaspsi:\",torch.mean(predictions_lambda_test-true_lambda_test),\"biaspsisd:\",torch.std(predictions_lambda_test-true_lambda_test))\n",
    "    print('MAEpsi:',torch.mean(abs(true_lambda_test-predictions_lambda_test)),'MAEpsisd:',torch.std(abs(true_lambda_test-predictions_lambda_test)))\n",
    "    print(\"EmpSD:\",torch.sqrt(torch.sum((prediction_logit_list-true_logits_test)**2)/len(prediction_logit_list)))\n",
    "\n",
    "    # print(torch.max(predictions_logits_test))\n",
    "\n",
    "    # print(predictions_lambda_test[1:60])\n",
    "    # print(max(predictions_lambda_test+quantile))\n",
    "    # print(max(true_lambda_test))\n",
    "    print(quantile)\n",
    "\n",
    "    within_confidence_interval = ((true_logits_test >= prediction_logit_list-quantile_list) & (true_logits_test <= prediction_logit_list+quantile_list))\n",
    "\n",
    "    # Step 2: Count the number of times true_probability is within the confidence interval\n",
    "    count_within_interval = within_confidence_interval.sum()\n",
    "\n",
    "    # Step 3: Calculate the frequency\n",
    "    frequency_within_interval = count_within_interval / len(predictions_logits_test) * 100  # as a percentage\n",
    "    print(\"CP:\",frequency_within_interval)\n",
    "\n",
    "    CI_length=torch.mean(abs(torch.exp(prediction_logit_list+quantile_list)-torch.exp(prediction_logit_list-quantile_list)))\n",
    "    print(\"AIL:\",CI_length)\n",
    "    CI_length_sd=torch.std(abs(torch.exp(prediction_logit_list+quantile_list)-torch.exp(prediction_logit_list-quantile_list)))\n",
    "    print(\"sd:\",CI_length_sd)\n",
    "else: \n",
    "\n",
    "    predictions_logits_train2 = network1(torch.tensor(X_sub2)).squeeze()  # Model predictions on training data\n",
    "    true_logits2=torch.tensor(f_1(X_sub2))\n",
    "    predictions_train2=1/(1+torch.exp(-predictions_logits_train2))\n",
    "    true_f2=1/(1+torch.exp(-true_logits2))\n",
    "    residuals2 = torch.abs(true_logits2 - predictions_logits_train2)  # Absolute residuals\n",
    "\n",
    "    # quantile=torch.quantile(torch.cat((residuals1,residuals2),dim=0),0.95)\n",
    "\n",
    "    \n",
    "    quantile = torch.quantile(residuals2, (1 - 0.05))\n",
    "\n",
    "\n",
    "    predictions_logits_test=network1(torch.tensor(xtest)).squeeze()\n",
    "    # predictions_logits_test=(network2(torch.tensor(xtest)).squeeze()+network2(torch.tensor(xtest)).squeeze())/2\n",
    "    true_logits_test=torch.tensor(f_1(xtest))\n",
    "    true_f_test=1/(1+torch.exp(-true_logits_test))\n",
    "    predictions_test=1/(1+torch.exp(-predictions_logits_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"biasf:\",torch.mean(prediction_logit_list-true_logits_test),\"biasf:\",torch.std(predictions_logits_test-true_logits_test))    \n",
    "    print(\"MAEf:\",torch.mean(abs(predictions_logits_test-true_logits_test)),\"MAEfsd:\",torch.std(abs(prediction_logit_list-true_logits_test)))     \n",
    "    print(\"biaspsi:\",torch.mean(predictions_test-true_f_test),\"biaspsisd:\",torch.std(predictions_test-true_f_test))\n",
    "    print('MAEpsi:',(torch.mean(abs(true_f_test-predictions_test))),'MAEpsisd:',(torch.std(abs(true_f_test-predictions_test))))\n",
    "    # print('MAEpsisd:',(torch.std(abs(true_f_test-predictions_test))))\n",
    "    print(\"EpmSD:\",torch.sqrt(torch.sum((predictions_logits_test-true_logits_test)**2)/len(predictions_logits_test)))\n",
    "\n",
    "    within_confidence_interval = ((true_logits_test >= predictions_logits_test-quantile) & (true_logits_test <= predictions_logits_test+quantile))\n",
    "\n",
    "    # within_confidence_interval = ((sigmoid(true_logits_test) >= sigmoid(predictions_logits_test)-quantile) & (sigmoid(true_logits_test) <= sigmoid(predictions_logits_test)+quantile))\n",
    "\n",
    "    # Step 2: Count the number of times true_probability is within the confidence interval\n",
    "    count_within_interval = within_confidence_interval.sum()\n",
    "\n",
    "    # Step 3: Calculate the frequency\n",
    "    frequency_within_interval = count_within_interval / len(predictions_logits_test) * 100  # as a percentage\n",
    "    print(\"CP:\",frequency_within_interval)\n",
    "    # print(quantile)\n",
    "    CI_length=torch.mean(abs(1/(1+torch.exp(-predictions_logits_test+quantile))-1/(1+torch.exp(-predictions_logits_test-quantile))))\n",
    "    print(\"AIC:\",CI_length)\n",
    "    CI_length_sd=torch.std(abs(1/(1+torch.exp(-predictions_logits_test+quantile))-1/(1+torch.exp(-predictions_logits_test-quantile))))\n",
    "    print(\"sd:\",CI_length_sd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
